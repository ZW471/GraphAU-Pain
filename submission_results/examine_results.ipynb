{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T16:42:04.685560Z",
     "start_time": "2025-01-18T16:42:03.376737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_multiple_inputs(input_file_output_folder_pairs, cate_num=3):\n",
    "    \"\"\"\n",
    "    Processes multiple input file pairs, computes the classification report and confusion matrix,\n",
    "    and saves the outputs (confusion matrix image and classification report CSV) to the respective output folders.\n",
    "\n",
    "    Args:\n",
    "        input_file_output_folder_pairs (list of tuples): A list where each tuple contains:\n",
    "            - First element: List of two file paths [predictions_file, labels_file].\n",
    "            - Second element: Output folder path for saving results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define the column headers and label mapping\n",
    "    if cate_num == 3:\n",
    "        columns = ['Mild Pain', 'No Pain', 'Pain']\n",
    "        label_mapping = {0: 'No Pain', 1: 'Mild', 2: 'Obvious'}\n",
    "    elif cate_num == 4:\n",
    "        columns = ['Mild Pain', 'No Pain', 'Pain', 'Obvious']\n",
    "        label_mapping = {0: 'No Pain', 1: 'Weak', 2: 'Mild', 3: 'Strong'}\n",
    "\n",
    "    for file_pair in input_file_output_folder_pairs:\n",
    "        # Unpack each tuple\n",
    "        input_files, output_folder = file_pair\n",
    "        predictions_file = input_files[0]\n",
    "        labels_file = input_files[1]\n",
    "\n",
    "        # Ensure the output folder exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Read and process labels\n",
    "        with open(labels_file, 'r') as lab:\n",
    "            labels = lab.readlines()\n",
    "            labels = [x.strip() for x in labels]\n",
    "            labels = [x.split(' ') for x in labels]\n",
    "            labels = pd.DataFrame(labels, columns=columns)\n",
    "            labels = labels.astype(int).idxmax(axis=1)  # Convert one-hot to class labels\n",
    "\n",
    "        # Read and process predictions\n",
    "        with open(predictions_file, 'r') as pred:\n",
    "            predictions = pred.readlines()\n",
    "            predictions = [x.strip() for x in predictions]\n",
    "            predictions = [x.split(' ') for x in predictions]\n",
    "            predictions = pd.DataFrame(predictions, columns=columns)\n",
    "            predictions = predictions.astype(int).idxmax(axis=1)  # Convert one-hot to class labels\n",
    "\n",
    "        # Generate and save classification report\n",
    "        report = classification_report(labels, predictions, target_names=label_mapping.values(), output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_csv_path = os.path.join(output_folder, 'classification_report.csv')\n",
    "        report_df.to_csv(report_csv_path)\n",
    "        print(f\"Classification report saved at: {report_csv_path}\")\n",
    "\n",
    "        # Generate and save confusion matrix\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        cm_original = cm.copy()  # Keep a copy of the original amounts\n",
    "        cm += 1\n",
    "        cm = np.log10(cm)\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='.2f', cmap=sns.light_palette(\"royalblue\", as_cmap=True),\n",
    "                         xticklabels=label_mapping.values(), yticklabels=label_mapping.values(), annot_kws={\"size\": 30},\n",
    "                         square=True, vmin=0, vmax=4, cbar=False)\n",
    "\n",
    "        # Add original amounts under the heatmap annotations (slightly lower and with white background)\n",
    "        for i in range(cm_original.shape[0]):\n",
    "            for j in range(cm_original.shape[1]):\n",
    "                text = ax.text(j + 0.5, i + 0.8, f\"({cm_original[i, j]})\",\n",
    "                               fontsize=18, color=\"black\", ha='center', va='center', clip_on=True,\n",
    "                               bbox=dict(boxstyle=\"round,pad=0.2\", edgecolor='none', facecolor='white', alpha=0.8))\n",
    "\n",
    "        # Customizing visualization\n",
    "        plt.xlabel(\"Predicted\", fontsize=20, fontweight='bold')\n",
    "        plt.ylabel(\"Actual\", fontsize=20, fontweight='bold')\n",
    "        plt.xticks(fontsize=20)  # Set x-ticklabel font size\n",
    "        plt.yticks(fontsize=20)  # Set y-ticklabel font size\n",
    "        # No title\n",
    "        confusion_matrix_path = os.path.join(output_folder, 'confusion_matrix.png')\n",
    "        plt.savefig(confusion_matrix_path, bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()  # Close the plot to avoid memory issues\n",
    "        print(f\"Confusion matrix saved at: {confusion_matrix_path}\")\n",
    "\n",
    "# Example list of input file pairs\n",
    "input_file_output_folder_pairs = [\n",
    "    (['bb.txt', '../data/UNBC/list/final_3/UNBC_test_pspi_fold1.txt'], 'only backbone'),\n",
    "    (['no gnn.txt', '../data/UNBC/list/final_3/UNBC_test_pspi_fold1.txt'], 'no gnn'),\n",
    "    (['pain_predictions.txt', '../data/UNBC/list/final_3/UNBC_test_pspi_fold1.txt'], 'full'),\n",
    "    (['full gr.txt', '../data/UNBC/list/final_3/UNBC_test_pspi_fold1.txt'], 'full + graph representation'),\n",
    "    (['full gr no sft.txt', '../data/UNBC/list/final_3/UNBC_test_pspi_fold1.txt'], 'full no sft'),\n",
    "]\n",
    "\n",
    "# Call the function\n",
    "process_multiple_inputs(input_file_output_folder_pairs)\n",
    "\n",
    "process_multiple_inputs(\n",
    "    [(['4cat.txt', '../data/UNBC/list/final_4/UNBC_test_pspi_fold1.txt'], '4 cat')], cate_num=4\n",
    ")"
   ],
   "id": "3cb6ea07d8c52456",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report saved at: only backbone\\classification_report.csv\n",
      "Confusion matrix saved at: only backbone\\confusion_matrix.png\n",
      "Classification report saved at: no gnn\\classification_report.csv\n",
      "Confusion matrix saved at: no gnn\\confusion_matrix.png\n",
      "Classification report saved at: full\\classification_report.csv\n",
      "Confusion matrix saved at: full\\confusion_matrix.png\n",
      "Classification report saved at: full + graph representation\\classification_report.csv\n",
      "Confusion matrix saved at: full + graph representation\\confusion_matrix.png\n",
      "Classification report saved at: full no sft\\classification_report.csv\n",
      "Confusion matrix saved at: full no sft\\confusion_matrix.png\n",
      "Classification report saved at: 4 cat\\classification_report.csv\n",
      "Confusion matrix saved at: 4 cat\\confusion_matrix.png\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T18:08:24.148409Z",
     "start_time": "2025-01-18T18:08:24.119394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder names and abbreviated model names\n",
    "folders = {'full + graph representation': 'Full', 'full': 'W/o graph rep.', 'no gnn': 'W/o GNN','only backbone': 'Only ResNet' }\n",
    "categories = ['No Pain', 'Mild', 'Obvious']\n",
    "# Initialize an empty dictionary to store the metrics\n",
    "model_metrics = {}\n",
    "\n",
    "# Loop through each folder to extract required metrics (F1-score, recall, and precision)\n",
    "for folder, abbrev in folders.items():\n",
    "    report_path = os.path.join(folder, 'classification_report.csv')\n",
    "    if os.path.exists(report_path):\n",
    "        report = pd.read_csv(report_path, index_col=0)\n",
    "\n",
    "        # Get F1-scores, recall, and precision for each class\n",
    "        f1_scores = report.loc[['No Pain', 'Mild', 'Obvious'], 'f1-score'].values\n",
    "        recalls = report.loc[['No Pain', 'Mild', 'Obvious'], 'recall'].values\n",
    "        precisions = report.loc[['No Pain', 'Mild', 'Obvious'], 'precision'].values\n",
    "\n",
    "        # Save F1, recall, and precision for the current model\n",
    "        model_metrics[abbrev] = {'f1': f1_scores, 'recall': recalls, 'precision': precisions}\n",
    "\n",
    "# Calculate min and max for each category across all metrics\n",
    "min_max_values = {}\n",
    "for metric in ['f1', 'recall', 'precision']:\n",
    "    min_max_values[metric] = {i: (float('inf'), float('-inf')) for i in range(len(categories))}\n",
    "\n",
    "for model in model_metrics.values():\n",
    "    for metric in min_max_values.keys():\n",
    "        for i, value in enumerate(model[metric]):\n",
    "            current_min, current_max = min_max_values[metric][i]\n",
    "            min_max_values[metric][i] = (min(current_min, value), max(current_max, value))\n",
    "\n",
    "# Calculate min and max values for the mean column\n",
    "mean_min, mean_max = float('inf'), float('-inf')\n",
    "\n",
    "# Compute min and max for the mean column across all models\n",
    "for model in model_metrics.values():\n",
    "    for metric in ['f1', 'recall', 'precision']:\n",
    "        mean_value = sum(model[metric]) / len(model[metric])\n",
    "        mean_min = min(mean_min, mean_value)\n",
    "        mean_max = max(mean_max, mean_value)\n",
    "\n",
    "# Helper function to apply color based on category-specific min-max value\n",
    "def color_cell(value, min_val, max_val, use_color=True):\n",
    "    if use_color:\n",
    "        if max_val == min_val:  # Handle case when all values are the same\n",
    "            normalized_value = 0.5  # Assign a neutral midpoint\n",
    "        else:\n",
    "            normalized_value = (value - min_val) / (max_val - min_val)\n",
    "        green_intensity = int(155 + 100 * normalized_value)\n",
    "        red_intensity = int(255 - 100 * normalized_value)\n",
    "        return f\"\\\\cellcolor[RGB]{{{red_intensity},{green_intensity},155}}{value:.1f}\"\n",
    "    else:\n",
    "        return f\"{value:.1f}\"  # No color applied\n",
    "\n",
    "# Function to prepare the LaTeX table\n",
    "def create_latex_table(use_color=True):\n",
    "    # Prepare the values to be inserted into the LaTeX table\n",
    "    table_data = []\n",
    "    metrics = ['F1', 'Recall', 'Precision']\n",
    "\n",
    "    # Loop through the models and metrics to create rows\n",
    "    for abbrev in folders.values():\n",
    "        for metric_idx, metric in enumerate(metrics):\n",
    "            row = [abbrev if metric_idx == 0 else \"\", metric]  # Add model name for the first metric row only\n",
    "            mean_value = 0  # Initialize mean value\n",
    "            metric_values = []\n",
    "\n",
    "            for i in range(len(model_metrics[abbrev][metric.lower()])):\n",
    "                value = float(model_metrics[abbrev][metric.lower()][i]) * 100\n",
    "                metric_values.append(value)\n",
    "                min_val, max_val = min_max_values[metric.lower()][i]\n",
    "                row.append(color_cell(value, min_val * 100, max_val * 100, use_color))  # Apply cell coloring if enabled\n",
    "\n",
    "            mean_value = sum(metric_values) / len(metric_values)  # Compute mean value\n",
    "            row.append(color_cell(mean_value, mean_min * 100, mean_max * 100, use_color))  # Apply cell coloring to the mean column\n",
    "            table_data.append(row)\n",
    "\n",
    "    # Construct the LaTeX table with resizebox\n",
    "    latex_table = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\resizebox{\\\\columnwidth}{!}{%\\n\\\\begin{tabular}{l|l|\" + \"|\".join([\"c\"] * (len(categories) + 1)) + \"}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\textbf{Model} & \\\\textbf{Metric} & \" + \" & \".join([f\"\\\\textbf{{{category}}}\" for category in categories]) + \" & \\\\textbf{Mean} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Add rows to the LaTeX table\n",
    "    for row in table_data:\n",
    "        latex_table += f\"\\\\multirow{{3}}{{*}}{{{row[0]}}} & {row[1]} & \" if row[0] != \"\" else f\" & {row[1]} & \"\n",
    "        latex_table += \" & \".join(row[2:]) + \" \\\\\\\\\\n\"\n",
    "        if row[1] == \"Precision\":  # Add \\hline after the last row of each model\n",
    "            latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    latex_table += \"\\\\end{tabular}%\\n}\\n\"\n",
    "    latex_table += \"\\\\caption{Three-category classification results with separate columns for models and metrics. F1, recall, precision, and their mean values are shown for No Pain, Mild Pain, and Pain categories.}\\n\"\n",
    "    latex_table += \"\\\\label{tab:all_models_results}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "# Generate the LaTeX table\n",
    "print(create_latex_table(use_color=False))  # Set to False to disable color"
   ],
   "id": "b6d40a89a9c6ad9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\resizebox{\\columnwidth}{!}{%\n",
      "\\begin{tabular}{l|l|c|c|c|c}\n",
      "\\hline\n",
      "\\textbf{Model} & \\textbf{Metric} & \\textbf{No Pain} & \\textbf{Mild} & \\textbf{Obvious} & \\textbf{Mean} \\\\\n",
      "\\hline\n",
      "\\multirow{3}{*}{Full} & F1 & 93.1 & 51.2 & 54.3 & 66.2 \\\\\n",
      " & Recall & 92.9 & 52.5 & 51.0 & 65.4 \\\\\n",
      " & Precision & 93.4 & 50.0 & 58.1 & 67.2 \\\\\n",
      "\\hline\n",
      "\\multirow{3}{*}{W/o graph rep.} & F1 & 87.7 & 46.4 & 55.2 & 63.1 \\\\\n",
      " & Recall & 80.8 & 73.4 & 51.0 & 68.4 \\\\\n",
      " & Precision & 95.9 & 33.9 & 60.2 & 63.4 \\\\\n",
      "\\hline\n",
      "\\multirow{3}{*}{W/o GNN} & F1 & 86.4 & 30.7 & 4.0 & 40.3 \\\\\n",
      " & Recall & 79.9 & 43.2 & 10.2 & 44.4 \\\\\n",
      " & Precision & 93.9 & 23.8 & 2.5 & 40.1 \\\\\n",
      "\\hline\n",
      "\\multirow{3}{*}{Only ResNet} & F1 & 70.3 & 31.0 & 4.2 & 35.2 \\\\\n",
      " & Recall & 56.5 & 68.3 & 15.3 & 46.7 \\\\\n",
      " & Precision & 92.8 & 20.1 & 2.5 & 38.5 \\\\\n",
      "\\hline\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\caption{Three-category classification results with separate columns for models and metrics. F1, recall, precision, and their mean values are shown for No Pain, Mild Pain, and Pain categories.}\n",
      "\\label{tab:all_models_results}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:57:19.043307Z",
     "start_time": "2025-01-08T20:57:19.017789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def classification_results_to_dataframe(csv_path):\n",
    "    \"\"\"\n",
    "    Reads a classification report CSV file and outputs a DataFrame with the specified format.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the classification report CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [\"Metric\", each category, \"Overall\"]\n",
    "                      and rows [\"F1\", \"Recall\", \"Precision\", \"Accuracy\"].\n",
    "    \"\"\"\n",
    "    # Read classification report from the CSV\n",
    "    report = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    # Extract the required metrics for each pain level category\n",
    "    categories = [col for col in report.index if col not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "    metrics = ['f1-score', 'recall', 'precision']\n",
    "    overall = report.loc['accuracy', 'precision'] * 100 if 'accuracy' in report.index else None  # Accuracy if present\n",
    "\n",
    "    data = []\n",
    "    for metric in metrics:\n",
    "        row = [metric.capitalize()] + [report.at[category, metric] * 100 for category in categories]\n",
    "        overall_metric = report.loc['macro avg', metric] * 100 if 'macro avg' in report.index else None\n",
    "        data.append(row + [overall_metric])\n",
    "\n",
    "    # Include accuracy row\n",
    "    if overall is not None:\n",
    "        accuracy_row = ['Accuracy'] + [None] * len(categories) + [overall]\n",
    "        data.append(accuracy_row)\n",
    "\n",
    "    columns = ['Metric'] + categories + ['Overall']\n",
    "    results_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "cat4 = classification_results_to_dataframe('../submission_results/4 cat/classification_report.csv')\n",
    "cat3 = classification_results_to_dataframe('../submission_results/full + graph representation/classification_report.csv')\n",
    "cat3"
   ],
   "id": "1c65f1c339eb8105",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Metric    No Pain       Mild    Obvious    Overall\n",
       "0   F1-score  93.101749  51.185250  54.347826  66.211609\n",
       "1     Recall  92.850123  52.475248  51.020408  65.448593\n",
       "2  Precision  93.354743  49.957155  58.139535  67.150478\n",
       "3   Accuracy        NaN        NaN        NaN  87.613649"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>No Pain</th>\n",
       "      <th>Mild</th>\n",
       "      <th>Obvious</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>93.101749</td>\n",
       "      <td>51.185250</td>\n",
       "      <td>54.347826</td>\n",
       "      <td>66.211609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>92.850123</td>\n",
       "      <td>52.475248</td>\n",
       "      <td>51.020408</td>\n",
       "      <td>65.448593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>93.354743</td>\n",
       "      <td>49.957155</td>\n",
       "      <td>58.139535</td>\n",
       "      <td>67.150478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.613649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:57:50.874852Z",
     "start_time": "2025-01-08T20:57:50.857348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_latex_table_from_dataframe(df):\n",
    "    \"\"\"\n",
    "    Convert a classification results DataFrame into a LaTeX table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing classification metrics with columns\n",
    "                           [\"Metric\", categories, \"Overall\"].\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX table string with merged cells for \"Accuracy\" across pain levels.\n",
    "    \"\"\"\n",
    "\n",
    "    cat_num = len(df.columns[1:-1])\n",
    "\n",
    "    latex_table = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\resizebox{\\\\columnwidth}{!}{%\\n\\\\begin{tabular}{l|\" + \"c|\" * (df.shape[1] - 2) + \"c}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\textbf{Metric} & \" + \" & \".join([f\"\\\\textbf{{{col}}}\" for col in df.columns[1:-1]]) + \" & \\\\textbf{Overall} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"Metric\"] == \"Accuracy\":\n",
    "            merged_pain_levels = \" & \".join([\"\\\\multicolumn{1}{c|}{}\" for _ in range(len(df.columns) - 2)])  # Empty merged cells\n",
    "            latex_table += f\"\\\\multirow{{1}}{{*}}{{Accuracy}} & {merged_pain_levels} & {row['Overall']:.2f} \\\\\\\\\\n\"\n",
    "        else:\n",
    "            values = \" & \".join([f\"{row[col]:.2f}\" if not pd.isna(row[col]) else \"\" for col in df.columns[1:]])\n",
    "            latex_table += f\"{row['Metric']} & {values} \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\end{tabular}%\\n}\\n\"\n",
    "    latex_table += \"\\\\caption{Classification metrics in LaTeX table format with merged accuracy cells.}\\n\"\n",
    "    latex_table += \"\\\\label{tab:classification_results_cat\" + str(cat_num) + \"}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "# Example usage\n",
    "latex_table_3cat = create_latex_table_from_dataframe(cat3)\n",
    "latex_table_4cat = create_latex_table_from_dataframe(cat4)\n",
    "print(latex_table_3cat)\n",
    "print(latex_table_4cat)"
   ],
   "id": "5684d456068bcd56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\resizebox{\\columnwidth}{!}{%\n",
      "\\begin{tabular}{l|c|c|c|c}\n",
      "\\hline\n",
      "\\textbf{Metric} & \\textbf{No Pain} & \\textbf{Mild} & \\textbf{Obvious} & \\textbf{Overall} \\\\\n",
      "\\hline\n",
      "F1-score & 93.10 & 51.19 & 54.35 & 66.21 \\\\\n",
      "Recall & 92.85 & 52.48 & 51.02 & 65.45 \\\\\n",
      "Precision & 93.35 & 49.96 & 58.14 & 67.15 \\\\\n",
      "\\multirow{1}{*}{Accuracy} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & 87.61 \\\\\n",
      "\\hline\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\caption{Classification metrics in LaTeX table format with merged accuracy cells.}\n",
      "\\label{tab:classification_results_cat3}\n",
      "\\end{table}\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\resizebox{\\columnwidth}{!}{%\n",
      "\\begin{tabular}{l|c|c|c|c|c}\n",
      "\\hline\n",
      "\\textbf{Metric} & \\textbf{No Pain} & \\textbf{Weak} & \\textbf{Mild} & \\textbf{Strong} & \\textbf{Overall} \\\\\n",
      "\\hline\n",
      "F1-score & 91.32 & 0.00 & 47.46 & 36.51 & 43.83 \\\\\n",
      "Recall & 87.09 & 0.00 & 76.91 & 47.84 & 52.96 \\\\\n",
      "Precision & 95.99 & 0.00 & 34.32 & 29.52 & 39.96 \\\\\n",
      "\\multirow{1}{*}{Accuracy} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & 82.40 \\\\\n",
      "\\hline\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\caption{Classification metrics in LaTeX table format with merged accuracy cells.}\n",
      "\\label{tab:classification_results_cat4}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T21:09:31.741873Z",
     "start_time": "2025-01-08T21:09:31.732853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_custom_latex_table(df):\n",
    "    \"\"\"\n",
    "    Create a LaTeX table with specified metrics for two models: GraphAU-Pain and GLA-CNN.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing classification metrics.\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX table string.\n",
    "    \"\"\"\n",
    "    # Extract values for GraphAU-Pain model from cat4 DataFrame\n",
    "    graphau_pain_values = [\n",
    "        df.loc[df['Metric'] == 'Accuracy', 'Overall'].values[0],\n",
    "        df.loc[df['Metric'] == 'F1-score', 'Overall'].values[0],\n",
    "        df.loc[df['Metric'] == 'Recall', 'Overall'].values[0],\n",
    "        df.loc[df['Metric'] == 'Precision', 'Overall'].values[0]\n",
    "    ]\n",
    "\n",
    "    # Define values for GLA-CNN\n",
    "    gla_cnn_values = [56.45, 36.52, 34.08, 43.23]\n",
    "\n",
    "    # Compute difference and define up/down arrows\n",
    "    differences = [(gp - gc, \"↑\" if gp > gc else \"↓\") for gp, gc in zip(graphau_pain_values, gla_cnn_values)]\n",
    "\n",
    "    # Start constructing the LaTeX table using resizebox for column width\n",
    "    latex_table = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\resizebox{\\\\columnwidth}{!}{%\\n\\\\begin{tabular}{l|c|c|c|c}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\textbf{Model} & \\\\textbf{Accuracy} & \\\\textbf{F1-score} & \\\\textbf{Recall} & \\\\textbf{Precision} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Add rows for each model\n",
    "    latex_table += f\"GraphAU-Pain & {graphau_pain_values[0]:.2f} ({differences[0][1]}{abs(differences[0][0]):.2f}) & \" \\\n",
    "                   f\"{graphau_pain_values[1]:.2f} ({differences[1][1]}{abs(differences[1][0]):.2f}) & \" \\\n",
    "                   f\"{graphau_pain_values[2]:.2f} ({differences[2][1]}{abs(differences[2][0]):.2f}) & \" \\\n",
    "                   f\"{graphau_pain_values[3]:.2f} ({differences[3][1]}{abs(differences[3][0]):.2f}) \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += f\"GLA-CNN & {gla_cnn_values[0]:.2f} & {gla_cnn_values[1]:.2f} & {gla_cnn_values[2]:.2f} & {gla_cnn_values[3]:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\end{tabular}%\\n}\\n\\\\caption{Custom model comparison for Accuracy, F1-score, Recall, and Precision.}\\n\"\n",
    "    latex_table += \"\\\\label{tab:model_comparison}\\n\\\\end{table}\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "# Example usage\n",
    "custom_table = create_custom_latex_table(cat4)\n",
    "print(custom_table)"
   ],
   "id": "94415f079f284485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\resizebox{\\columnwidth}{!}{%\n",
      "\\begin{tabular}{l|c|c|c|c}\n",
      "\\hline\n",
      "\\textbf{Model} & \\textbf{Accuracy} & \\textbf{F1-score} & \\textbf{Recall} & \\textbf{Precision} \\\\\n",
      "\\hline\n",
      "GraphAU-Pain & 82.40 (↑25.95) & 43.83 (↑7.31) & 52.96 (↑18.88) & 39.96 (↓3.27) \\\\\n",
      "GLA-CNN & 56.45 & 36.52 & 34.08 & 43.23 \\\\\n",
      "\\hline\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\caption{Custom model comparison for Accuracy, F1-score, Recall, and Precision.}\n",
      "\\label{tab:model_comparison}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c244c526a530d882"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
